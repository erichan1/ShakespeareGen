{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase\n",
      "Total Sequences: 93633\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "# load text\n",
    "fname = 'shakespeare_filtered.txt'\n",
    "raw_text = load_doc(fname)\n",
    "#print(raw_text)\n",
    " \n",
    "# clean\n",
    "tokens = raw_text.split()\n",
    "#print(tokens)\n",
    "raw_text = ' '.join(tokens)\n",
    " \n",
    "# organize into sequences of characters\n",
    "length = 40\n",
    "sequences = list()\n",
    "for i in range(length, len(raw_text)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = raw_text[i-length:i+1]\n",
    "\t# store\n",
    "\tsequences.append(seq)\n",
    "print(sequences[0])\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    " \n",
    "# save sequences to file\n",
    "out_filename = 'char_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 61\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load\n",
    "in_filename = 'char_sequences.txt'\n",
    "raw_text = load_doc(in_filename)\n",
    "lines = raw_text.split('\\n')\n",
    "\n",
    "# integer encode sequences of characters\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "sequences = list()\n",
    "for line in lines:\n",
    "\t# integer encode line\n",
    "\tencoded_seq = [mapping[char] for char in line]\n",
    "\t# store\n",
    "\tsequences.append(encoded_seq)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(mapping)\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 71\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 100)               68800     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 71)                7171      \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 71)                0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 71)                0         \n",
      "=================================================================\n",
      "Total params: 75,971\n",
      "Trainable params: 75,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "94141/94141 [==============================] - 1242s 13ms/step - loss: 2.1364 - acc: 0.3845\n",
      "Epoch 2/20\n",
      "94141/94141 [==============================] - 792s 8ms/step - loss: 1.8078 - acc: 0.4582\n",
      "Epoch 3/20\n",
      "94141/94141 [==============================] - 759s 8ms/step - loss: 1.6800 - acc: 0.4900\n",
      "Epoch 4/20\n",
      "94141/94141 [==============================] - 771s 8ms/step - loss: 1.6016 - acc: 0.5096\n",
      "Epoch 5/20\n",
      "94141/94141 [==============================] - 769s 8ms/step - loss: 1.5419 - acc: 0.5252\n",
      "Epoch 6/20\n",
      "94141/94141 [==============================] - 718s 8ms/step - loss: 1.4949 - acc: 0.5361\n",
      "Epoch 7/20\n",
      "94141/94141 [==============================] - 724s 8ms/step - loss: 1.4530 - acc: 0.5473\n",
      "Epoch 8/20\n",
      "94141/94141 [==============================] - 779s 8ms/step - loss: 1.4173 - acc: 0.5575\n",
      "Epoch 9/20\n",
      "94141/94141 [==============================] - 5088s 54ms/step - loss: 1.3856 - acc: 0.5650\n",
      "Epoch 10/20\n",
      "94141/94141 [==============================] - 691s 7ms/step - loss: 1.3559 - acc: 0.5732\n",
      "Epoch 11/20\n",
      "94141/94141 [==============================] - 614s 7ms/step - loss: 1.3293 - acc: 0.5816\n",
      "Epoch 12/20\n",
      "94141/94141 [==============================] - 328s 3ms/step - loss: 1.3039 - acc: 0.5872\n",
      "Epoch 13/20\n",
      "94141/94141 [==============================] - 322s 3ms/step - loss: 1.2804 - acc: 0.5966\n",
      "Epoch 14/20\n",
      "94141/94141 [==============================] - 333s 4ms/step - loss: 1.2578 - acc: 0.6013\n",
      "Epoch 15/20\n",
      "94141/94141 [==============================] - 365s 4ms/step - loss: 1.2384 - acc: 0.6062\n",
      "Epoch 16/20\n",
      "94141/94141 [==============================] - 378s 4ms/step - loss: 1.2193 - acc: 0.6128\n",
      "Epoch 17/20\n",
      "94141/94141 [==============================] - 481s 5ms/step - loss: 1.2029 - acc: 0.6176\n",
      "Epoch 18/20\n",
      "94141/94141 [==============================] - 404s 4ms/step - loss: 1.1866 - acc: 0.6222\n",
      "Epoch 19/20\n",
      "94141/94141 [==============================] - 408s 4ms/step - loss: 1.1712 - acc: 0.6252\n",
      "Epoch 20/20\n",
      "94141/94141 [==============================] - 351s 4ms/step - loss: 1.1559 - acc: 0.6314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb77b24a90>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Lambda\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load\n",
    "in_filename = 'char_sequences.txt'\n",
    "raw_text = load_doc(in_filename)\n",
    "lines = raw_text.split('\\n')\n",
    "\n",
    "# integer encode sequences of characters\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "sequences = list()\n",
    "for line in lines:\n",
    "\t# integer encode line\n",
    "\tencoded_seq = [mapping[char] for char in line]\n",
    "\t# store\n",
    "\tsequences.append(encoded_seq)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(mapping)\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# define model\n",
    "temp = 0.25\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Lambda(lambda x: x / temp))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, epochs=20, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall I compare thee to a summer's day?\n",
      "'s alse the summer that thou shouldst bear, My sear have I will be it the world beare, The summer that the sweet summer that thee show, The likened buries the praise that the stain: So ill in the far the bright do not be song. 105 I like a wortated with the time and state, And therefore that the summer that thou art, In me that thou shouldst that the sweet state, And therefore that the summer that thou art, In me that thou shouldst that the sweet state, And therefore that the summer that thou art, In me that thou shouldst that the sweet state, And therefore that the summer that thou art, In me\n",
      "In me that thou shouldst that the sweet state, And therefore\n",
      "led the sweet summer that true, My dear strange that thou shouldst that the stre\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of characters\n",
    "\tfor _ in range(n_chars):\n",
    "\t\t# encode the characters as integers\n",
    "\t\tencoded = [mapping[char] for char in in_text]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# one hot encode\n",
    "\t\tencoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "\t\tencoded = encoded.reshape(-1, encoded.shape[1], encoded.shape[2])\n",
    "\t\t# predict character\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# reverse map integer to character\n",
    "\t\tout_char = ''\n",
    "\t\tfor char, index in mapping.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_char = char\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += char\n",
    "\treturn in_text\n",
    "\n",
    "seed_line = \"shall I compare thee to a summer's day?\\n\"\n",
    "\n",
    "# test start of poem\n",
    "print(generate_seq(model, mapping, 40, seed_line, 40*15))\n",
    "print(generate_seq(model, mapping, 40, 'In me that thou shouldst that the sweet state, And therefore\\n', 40*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
